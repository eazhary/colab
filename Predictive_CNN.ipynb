{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictive-CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/eazhary/colab/blob/master/Predictive_CNN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "GE_-7mE1cz53",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/umbertogriffo/Predictive-Maintenance-using-LSTM/master/Dataset/PM_test.txt -O PM_test.txt \n",
        "!wget -q https://raw.githubusercontent.com/umbertogriffo/Predictive-Maintenance-using-LSTM/master/Dataset/PM_train.txt -O PM_train.txt  \n",
        "!wget -q https://raw.githubusercontent.com/umbertogriffo/Predictive-Maintenance-using-LSTM/master/Dataset/PM_truth.txt -O PM_truth.txt\n",
        "\n",
        "# In[ ]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KViXxDRvc5zl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers.core import Activation\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPool1D, GlobalMaxPooling1D, Flatten\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzW8-BK4cgtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f1c7d32b-49f8-4358-cfda-f5c6efe37002"
      },
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "# define path to save model\n",
        "model_path = 'regression_model.h5'\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# read training data - It is the aircraft engine run-to-failure data.\n",
        "train_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\n",
        "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])\n",
        "\n",
        "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
        "test_df = pd.read_csv('PM_test.txt', sep=\" \", header=None)\n",
        "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('PM_truth.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "train_df.describe()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "##################################\n",
        "# Data Preprocessing\n",
        "##################################\n",
        "\n",
        "#######\n",
        "# TRAIN\n",
        "#######\n",
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification, \n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)\n",
        "#print(\"Train DF\", train_df.head())\n",
        "#train_df.to_csv('PredictiveManteinanceEngineTraining.csv', encoding='utf-8',index = None)\n",
        "\n",
        "######\n",
        "# TEST\n",
        "######\n",
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "#print(\"Test DF\",test_df.head())\n",
        "\n",
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "#print(\"RUL for TEST\",rul.head())\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "#print(\"Truth DF\", truth_df.head())\n",
        "truth_df.drop('more', axis=1, inplace=True)\n",
        "\n",
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
        "#print(\"Final Test\",test_df.head())\n",
        "#test_df.to_csv('PredictiveManteinanceEngineValidation.csv', encoding='utf-8',index = None)\n",
        "\n",
        "# pick a large window size of 50 cycles\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "sequence_length = 50\n",
        "\n",
        "# function to reshape features into (samples, time steps, features) \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    # for one id I put all the rows in a single matrix\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    # Iterate over two lists in parallel.\n",
        "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
        "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
        "    # 0 50 -> from row 0 to row 50\n",
        "    # 1 51 -> from row 1 to row 51\n",
        "    # 2 52 -> from row 2 to row 52\n",
        "    # ...\n",
        "    # 111 191 -> from row 111 to 191\n",
        "    \n",
        "    if num_elements>seq_length:\n",
        "        for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "            yield data_matrix[start:stop, :]\n",
        "    else:\n",
        "        return\n",
        "        \n",
        "# pick the feature columns \n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "print(sequence_cols, len(sequence_cols))\n",
        "# TODO for debug \n",
        "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
        "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
        "print(len(val))\n",
        "\n",
        "print(val[0])\n",
        "# generator for the sequences\n",
        "# transform each id of the train dataset in a sequence\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train_df['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "print(seq_array.shape)\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    # For one id I put all the labels in a single matrix.\n",
        "    # For example:\n",
        "    # [[1]\n",
        "    # [4]\n",
        "    # [1]\n",
        "    # [5]\n",
        "    # [9]\n",
        "    # ...\n",
        "    # [200]] \n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    # I have to remove the first seq_length labels\n",
        "    # because for one id the first sequence of seq_length size have as target\n",
        "    # the last label (the previus ones are discarded).\n",
        "    # All the next id's sequences will have associated step by step one label as target.\n",
        "    if num_elements>seq_length:\n",
        "        return data_matrix[seq_length:num_elements, :]\n",
        "    else:\n",
        "        return [None]\n",
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in train_df['id'].unique()]\n",
        "\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"Coefficient of Determination \n",
        "    \"\"\"\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "test_gen = (list(gen_sequence(test_df[test_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in test_df['id'].unique())\n",
        "test_list = list(test_gen)\n",
        "#print(test_list[0:3])\n",
        "test_list2= [x for x in test_list if x]\n",
        "#print(test_list2[0])\n",
        "# generate sequences and convert to numpy array\n",
        "test_array = np.concatenate(list(test_list2)).astype(np.float32)\n",
        "print(test_array.shape)\n",
        "\n",
        "ylabel_gen = [gen_labels(test_df[test_df['id']==id], sequence_length, ['RUL']) \n",
        "             for id in test_df['id'].unique()]\n",
        "yl = list(ylabel_gen)\n",
        "yl = [x for x in yl if type(x) is not list]\n",
        "#print(type(yl[0])\n",
        "#print(yl)\n",
        "ylabel_array = np.concatenate(yl).astype(np.float32)\n",
        "ylabel_array.shape\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['setting1', 'setting2', 'setting3', 'cycle_norm', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21'] 25\n",
            "142\n",
            "[[0.45977011 0.16666667 0.         ... 0.         0.71317829 0.7246617 ]\n",
            " [0.6091954  0.25       0.         ... 0.         0.66666667 0.73101353]\n",
            " [0.25287356 0.75       0.         ... 0.         0.62790698 0.62137531]\n",
            " ...\n",
            " [0.6091954  0.58333333 0.         ... 0.         0.62015504 0.65009666]\n",
            " [0.41954023 0.91666667 0.         ... 0.         0.71317829 0.76719138]\n",
            " [0.31609195 0.41666667 0.         ... 0.         0.5503876  0.71306269]]\n",
            "(15631, 50, 25)\n",
            "(8162, 50, 25)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8162, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "-jT6B59CSa9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "cellView": "both",
        "outputId": "ba8ab2d3-4f55-44ce-cd6a-91fccffc06ac"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "\n",
        "# Next, we build a deep network. \n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
        "# Dropout is also applied after each LSTM layer to control overfitting. \n",
        "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=25,kernel_size=3,activation='relu',input_shape=(sequence_length, nb_features) ))\n",
        "model.add(Dropout(0,2))\n",
        "model.add(MaxPooling1D())\n",
        "\n",
        "model.add(Conv1D(filters=25,kernel_size=3,activation='relu'))\n",
        "model.add(Dropout(0,2))\n",
        "model.add(MaxPooling1D())\n",
        "\n",
        "#model.add(Conv1D(filters=50,kernel_size=3,activation='relu'))\n",
        "#model.add(Dropout(0,2))\n",
        "#model.add(MaxPooling1D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0,2))\n",
        "model.add(Dense(1,activation='linear'))\n",
        "\n",
        "#model.add(Conv1D(filters=100,kernel_size=2, padding='same',activation='relu', input_shape=(sequence_length, nb_features) ))\n",
        "#model.add(Dropout(0,2))\n",
        "#model.add(Conv1D(filters=50,kernel_size=4, padding='same',activation='relu' ))\n",
        "#model.add(GlobalMaxPooling1D())\n",
        "#model.add(Dense(50, activation='relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "#model.add(Dense(1))\n",
        "#model.add(Activation('linear'))\n",
        "\n",
        "adam= keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='mse', optimizer=adam,metrics=['mae',r2_keras])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_28 (Conv1D)           (None, 48, 25)            1900      \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 48, 25)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_22 (MaxPooling (None, 24, 25)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 22, 25)            1900      \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 22, 25)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_23 (MaxPooling (None, 11, 25)            0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 275)               0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 275)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 276       \n",
            "=================================================================\n",
            "Total params: 4,076\n",
            "Trainable params: 4,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uGEqtw2WUYE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2434
        },
        "outputId": "4f0869d1-9de5-4a84-a961-8071d13f3a5f"
      },
      "cell_type": "code",
      "source": [
        "model_path='newmodel.h5'\n",
        "# fit the network\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05,validation_data=(test_array,ylabel_array), verbose=1,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='min'),\n",
        "                      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_delta=0, patience=3,cooldown=2, verbose=1, factor=0.5,mode='auto',min_lr=0.00001),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=1)]\n",
        "          )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15631 samples, validate on 8162 samples\n",
            "Epoch 1/100\n",
            "15631/15631 [==============================] - 1s 81us/step - loss: 2574.5642 - mean_absolute_error: 38.3374 - r2_keras: 0.2111 - val_loss: 3046.3493 - val_mean_absolute_error: 41.0678 - val_r2_keras: -0.6106\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3046.34935, saving model to newmodel.h5\n",
            "Epoch 2/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 1643.0345 - mean_absolute_error: 30.2796 - r2_keras: 0.4944 - val_loss: 1912.0434 - val_mean_absolute_error: 32.7473 - val_r2_keras: -0.0552\n",
            "\n",
            "Epoch 00002: val_loss improved from 3046.34935 to 1912.04340, saving model to newmodel.h5\n",
            "Epoch 3/100\n",
            "15631/15631 [==============================] - 1s 47us/step - loss: 1538.9151 - mean_absolute_error: 28.8938 - r2_keras: 0.5271 - val_loss: 2143.6087 - val_mean_absolute_error: 39.7008 - val_r2_keras: -0.6160\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1912.04340\n",
            "Epoch 4/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 1303.2660 - mean_absolute_error: 26.4293 - r2_keras: 0.5972 - val_loss: 1517.5564 - val_mean_absolute_error: 32.6063 - val_r2_keras: -0.0482\n",
            "\n",
            "Epoch 00004: val_loss improved from 1912.04340 to 1517.55642, saving model to newmodel.h5\n",
            "Epoch 5/100\n",
            "15631/15631 [==============================] - 1s 44us/step - loss: 1184.2717 - mean_absolute_error: 25.1045 - r2_keras: 0.6364 - val_loss: 2752.6796 - val_mean_absolute_error: 40.8613 - val_r2_keras: -0.4422\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1517.55642\n",
            "Epoch 6/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 1155.2439 - mean_absolute_error: 24.8526 - r2_keras: 0.6469 - val_loss: 1233.7639 - val_mean_absolute_error: 27.6032 - val_r2_keras: 0.2492\n",
            "\n",
            "Epoch 00006: val_loss improved from 1517.55642 to 1233.76386, saving model to newmodel.h5\n",
            "Epoch 7/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 1081.0274 - mean_absolute_error: 23.9734 - r2_keras: 0.6675 - val_loss: 1968.4460 - val_mean_absolute_error: 31.3708 - val_r2_keras: 0.0449\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1233.76386\n",
            "Epoch 8/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 1049.5481 - mean_absolute_error: 23.7436 - r2_keras: 0.6758 - val_loss: 1215.6663 - val_mean_absolute_error: 27.5312 - val_r2_keras: 0.2457\n",
            "\n",
            "Epoch 00008: val_loss improved from 1233.76386 to 1215.66628, saving model to newmodel.h5\n",
            "Epoch 9/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 986.8200 - mean_absolute_error: 22.8389 - r2_keras: 0.6976 - val_loss: 1632.0644 - val_mean_absolute_error: 27.9103 - val_r2_keras: 0.2169\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1215.66628\n",
            "Epoch 10/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 962.1360 - mean_absolute_error: 22.5276 - r2_keras: 0.7039 - val_loss: 1291.4714 - val_mean_absolute_error: 28.9408 - val_r2_keras: 0.1688\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1215.66628\n",
            "Epoch 11/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 922.3912 - mean_absolute_error: 22.0916 - r2_keras: 0.7170 - val_loss: 1194.8594 - val_mean_absolute_error: 26.5600 - val_r2_keras: 0.2867\n",
            "\n",
            "Epoch 00011: val_loss improved from 1215.66628 to 1194.85943, saving model to newmodel.h5\n",
            "Epoch 12/100\n",
            "15631/15631 [==============================] - 1s 44us/step - loss: 911.4644 - mean_absolute_error: 21.9690 - r2_keras: 0.7194 - val_loss: 2925.3186 - val_mean_absolute_error: 42.3410 - val_r2_keras: -0.5431\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1194.85943\n",
            "Epoch 13/100\n",
            "15631/15631 [==============================] - 1s 47us/step - loss: 886.5241 - mean_absolute_error: 21.4145 - r2_keras: 0.7281 - val_loss: 1861.0630 - val_mean_absolute_error: 32.5396 - val_r2_keras: 0.0449\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1194.85943\n",
            "Epoch 14/100\n",
            "15631/15631 [==============================] - 1s 44us/step - loss: 878.9001 - mean_absolute_error: 21.3109 - r2_keras: 0.7303 - val_loss: 1516.6006 - val_mean_absolute_error: 32.5218 - val_r2_keras: -0.0566\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1194.85943\n",
            "Epoch 15/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 720.7611 - mean_absolute_error: 19.0879 - r2_keras: 0.7791 - val_loss: 1108.3698 - val_mean_absolute_error: 23.6985 - val_r2_keras: 0.3895\n",
            "\n",
            "Epoch 00015: val_loss improved from 1194.85943 to 1108.36980, saving model to newmodel.h5\n",
            "Epoch 16/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 711.0983 - mean_absolute_error: 18.8960 - r2_keras: 0.7817 - val_loss: 1440.8520 - val_mean_absolute_error: 30.2005 - val_r2_keras: -0.0332\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1108.36980\n",
            "Epoch 17/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 699.2769 - mean_absolute_error: 18.6751 - r2_keras: 0.7856 - val_loss: 1061.7807 - val_mean_absolute_error: 23.8219 - val_r2_keras: 0.4182\n",
            "\n",
            "Epoch 00017: val_loss improved from 1108.36980 to 1061.78074, saving model to newmodel.h5\n",
            "Epoch 18/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 697.1136 - mean_absolute_error: 18.6443 - r2_keras: 0.7854 - val_loss: 1235.8514 - val_mean_absolute_error: 24.1268 - val_r2_keras: 0.3687\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1061.78074\n",
            "Epoch 19/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 677.3171 - mean_absolute_error: 18.3231 - r2_keras: 0.7923 - val_loss: 1108.9300 - val_mean_absolute_error: 24.4684 - val_r2_keras: 0.3878\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1061.78074\n",
            "Epoch 20/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 691.4436 - mean_absolute_error: 18.5395 - r2_keras: 0.7880 - val_loss: 1125.7061 - val_mean_absolute_error: 25.2472 - val_r2_keras: 0.3581\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1061.78074\n",
            "Epoch 21/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 629.2098 - mean_absolute_error: 17.4514 - r2_keras: 0.8067 - val_loss: 1064.1781 - val_mean_absolute_error: 23.9432 - val_r2_keras: 0.3995\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1061.78074\n",
            "Epoch 22/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 629.0344 - mean_absolute_error: 17.4677 - r2_keras: 0.8067 - val_loss: 1327.6820 - val_mean_absolute_error: 25.1707 - val_r2_keras: 0.3295\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1061.78074\n",
            "Epoch 23/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 623.6638 - mean_absolute_error: 17.3297 - r2_keras: 0.8081 - val_loss: 1248.0542 - val_mean_absolute_error: 24.3300 - val_r2_keras: 0.3633\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1061.78074\n",
            "Epoch 24/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 623.2583 - mean_absolute_error: 17.3291 - r2_keras: 0.8086 - val_loss: 1136.0728 - val_mean_absolute_error: 25.6519 - val_r2_keras: 0.3012\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1061.78074\n",
            "Epoch 25/100\n",
            "15631/15631 [==============================] - 1s 46us/step - loss: 605.7522 - mean_absolute_error: 17.0250 - r2_keras: 0.8137 - val_loss: 1071.4692 - val_mean_absolute_error: 23.5962 - val_r2_keras: 0.3989\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1061.78074\n",
            "Epoch 26/100\n",
            "15631/15631 [==============================] - 1s 44us/step - loss: 602.8244 - mean_absolute_error: 16.9555 - r2_keras: 0.8152 - val_loss: 1079.9171 - val_mean_absolute_error: 23.8551 - val_r2_keras: 0.3881\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1061.78074\n",
            "Epoch 27/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 600.1048 - mean_absolute_error: 16.9074 - r2_keras: 0.8156 - val_loss: 1120.9985 - val_mean_absolute_error: 23.5577 - val_r2_keras: 0.3920\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1061.78074\n",
            "Epoch 28/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 598.8222 - mean_absolute_error: 16.8779 - r2_keras: 0.8158 - val_loss: 1143.6906 - val_mean_absolute_error: 23.4076 - val_r2_keras: 0.4028\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1061.78074\n",
            "Epoch 29/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 592.3954 - mean_absolute_error: 16.7441 - r2_keras: 0.8181 - val_loss: 1116.7381 - val_mean_absolute_error: 23.3522 - val_r2_keras: 0.4061\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1061.78074\n",
            "Epoch 30/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 590.1839 - mean_absolute_error: 16.7140 - r2_keras: 0.8189 - val_loss: 1111.7264 - val_mean_absolute_error: 23.3979 - val_r2_keras: 0.4050\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1061.78074\n",
            "Epoch 31/100\n",
            "15631/15631 [==============================] - 1s 44us/step - loss: 590.2188 - mean_absolute_error: 16.7219 - r2_keras: 0.8187 - val_loss: 1092.9599 - val_mean_absolute_error: 23.8361 - val_r2_keras: 0.3747\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1061.78074\n",
            "Epoch 32/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 589.0062 - mean_absolute_error: 16.6980 - r2_keras: 0.8188 - val_loss: 1133.7806 - val_mean_absolute_error: 25.0658 - val_r2_keras: 0.3185\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1061.78074\n",
            "Epoch 00032: early stopping\n",
            "dict_keys(['val_loss', 'val_mean_absolute_error', 'val_r2_keras', 'loss', 'mean_absolute_error', 'r2_keras', 'lr'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_viZ0nH7lezF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1815
        },
        "outputId": "0dc71591-f5c2-465e-ffcd-2865bdca7aa3"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# summarize history for R^2\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['r2_keras'])\n",
        "plt.plot(history.history['val_r2_keras'])\n",
        "plt.title('model r^2')\n",
        "plt.ylabel('R^2')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_r2.png\")\n",
        "\n",
        "# summarize history for MAE\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['mean_absolute_error'])\n",
        "plt.plot(history.history['val_mean_absolute_error'])\n",
        "plt.title('model MAE')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_mae.png\")\n",
        "\n",
        "# summarize history for Loss\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_regression_loss.png\")\n",
        "\n",
        "# training metrics\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nR^2: {}'.format(scores[2]))\n",
        "\n",
        "y_pred = model.predict(seq_array,verbose=1, batch_size=200)\n",
        "y_true = label_array\n",
        "\n",
        "test_set = pd.DataFrame(y_pred)\n",
        "test_set.to_csv('submit_train.csv', index = None)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "mymodel = keras.models.load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "scores = model.evaluate(test_array,ylabel_array, verbose=1, batch_size=200)\n",
        "y_pred= model.predict(test_array)\n",
        "print('\\nScore 0 {}'.format(scores[0]))\n",
        "print('\\nMAE: {}'.format(scores[1]))\n",
        "print('\\nR^2: {}'.format(scores[2]))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "ylabel_array.shape\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'truth':ylabel_array.reshape(-1), 'predict':y_pred.reshape(-1)})\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "df.plot(subplots=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15631 samples, validate on 8162 samples\n",
            "Epoch 1/100\n",
            "15631/15631 [==============================] - 1s 60us/step - loss: 2512.5252 - mean_absolute_error: 38.0570 - r2_keras: 0.2295 - val_loss: 2005.2278 - val_mean_absolute_error: 36.8631 - val_r2_keras: -0.3340\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2005.22779, saving model to newmodel.h5\n",
            "Epoch 2/100\n",
            "15631/15631 [==============================] - 1s 43us/step - loss: 1506.2119 - mean_absolute_error: 28.4056 - r2_keras: 0.5358 - val_loss: 1837.4163 - val_mean_absolute_error: 35.0454 - val_r2_keras: -0.2495\n",
            "\n",
            "Epoch 00002: val_loss improved from 2005.22779 to 1837.41626, saving model to newmodel.h5\n",
            "Epoch 3/100\n",
            "15631/15631 [==============================] - 1s 45us/step - loss: 1344.9566 - mean_absolute_error: 26.2887 - r2_keras: 0.5860 - val_loss: 1702.2529 - val_mean_absolute_error: 33.3105 - val_r2_keras: -0.1391\n",
            "\n",
            "Epoch 00003: val_loss improved from 1837.41626 to 1702.25287, saving model to newmodel.h5\n",
            "Epoch 4/100\n",
            "15000/15631 [===========================>..] - ETA: 0s - loss: 1291.6090 - mean_absolute_error: 25.6250 - r2_keras: 0.6024"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6e21bb7aba4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='min'),\n\u001b[1;32m      5\u001b[0m                       \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                        keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=1)]\n\u001b[0m\u001b[1;32m      7\u001b[0m           )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1249\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1251\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rsRvQ5mvVVK4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}